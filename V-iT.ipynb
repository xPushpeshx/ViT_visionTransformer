{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## break problem into subproblem \n",
    "VIT into \n",
    "* Input Block\n",
    "* Output \n",
    "* Layers\n",
    "* Block\n",
    "* Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x is an image in research paper , And capitals are MAtrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT works by\n",
    "- input image ---> Patches\n",
    "- patches     ---> Transformer\n",
    "- transformer ---> MLP\n",
    "- MLP         ---> Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAPE=224\n",
    "COLOR_CHANNEL=3\n",
    "PATCH_SIZE=16\n",
    "NUM_CLASSES=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  PATCH LAYER/BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INput shape:  (224, 224, 3)\n",
      "Output shape: Single 1D sequence of PAtches  (196, 768)\n"
     ]
    }
   ],
   "source": [
    "NO_OF_PATCHES=(SHAPE*SHAPE//PATCH_SIZE**2)\n",
    "INPUT_SHAPE=(SHAPE,SHAPE,COLOR_CHANNEL)\n",
    "OUTPUT_SHAPE=(NO_OF_PATCHES, PATCH_SIZE**2  * COLOR_CHANNEL)\n",
    "\n",
    "print(\"INput shape: \",INPUT_SHAPE)\n",
    "print(\"Output shape: Single 1D sequence of PAtches \",OUTPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input=torch.randn(*INPUT_SHAPE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([224, 224, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(in_channels=3, out_channels=768, kernel_size=PATCH_SIZE, stride=PATCH_SIZE, padding=0)\n",
    "conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 14, 14])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_out_patch=conv2d(test_input.permute(2,0,1))\n",
    "image_out_patch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_out_patch.requires_grad # makes params learnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before passing it to transformer we need to reshape it and flatten it\n",
    "flatten_image_in_transformer=nn.Flatten(start_dim=2, end_dim=3)(image_out_patch)\n",
    "flatten_image_in_transformer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_image_in_transformer=flatten_image_in_transformer.permute(0,2,1)\n",
    "flatten_image_in_transformer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self,\n",
    "                input_shape,\n",
    "                patch_size,\n",
    "                output_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        #create a patching layer\n",
    "        self.patcher=nn.Conv2d(in_channels=input_shape,\n",
    "                                out_channels=output_shape,\n",
    "                                kernel_size=patch_size,\n",
    "                                stride=patch_size,\n",
    "                                padding=0)\n",
    "        self.flatten=nn.Flatten(start_dim=2, end_dim=3)\n",
    "\n",
    "    def forward(self,x):\n",
    "        image_res=x.shape[-1]\n",
    "        assert image_res % PATCH_SIZE ==0 , \"Image resolution should be divisible by patch size\"\n",
    "\n",
    "        # forward pass\n",
    "        x_patched=self.patcher(x)\n",
    "        x_flattened=self.flatten(x_patched)\n",
    "\n",
    "        return x_flattened.permute(0,2,1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_patch=PatchEmbedding(input_shape=COLOR_CHANNEL,\n",
    "                            patch_size=PATCH_SIZE,\n",
    "                            output_shape=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input=torch.randn(1,3,SHAPE,SHAPE).to(device)\n",
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 768])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_embedding=create_patch(test_input)\n",
    "patch_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in ViT there is a catch the 1st embedding that you create is a positional embedding which you create manually and it doesnt takes any input from flattening of the Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating that positional encoding\n",
    "batch_size=1    \n",
    "\n",
    "class_token=nn.Parameter(torch.ones(batch_size,1,768),requires_grad=True)\n",
    "class_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addition_of_pos_embedding=torch.cat((class_token,patch_embedding),dim=1)\n",
    "addition_of_pos_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSA BLOCK (Multi head self Attention Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSA(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim,\n",
    "                 no_heads,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        self.layer_norm=nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        self.multi_head_attention=nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                        num_heads=no_heads,\n",
    "                                                        dropout=dropout,\n",
    "                                                        batch_first=True)  \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.layer_norm(x)\n",
    "        attention_output, _ = self.multi_head_attention(x, x, x,need_weights=False)\n",
    "\n",
    "        return attention_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlsa=MSA(embedding_dim=768,no_heads=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlsa(addition_of_pos_embedding).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,embedding_dim,\n",
    "                 mlp_size,dropout=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm=nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "        self.mlp=nn.Sequential(\n",
    "            nn.Linear(embedding_dim,mlp_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_size,embedding_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.layer_norm(x)\n",
    "        x=self.mlp(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim,\n",
    "                 no_heads,\n",
    "                 mlp_size,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.msa=MSA(embedding_dim=embedding_dim,\n",
    "                     no_heads=no_heads,\n",
    "                     dropout=dropout)\n",
    "\n",
    "        self.mlp=MLP(embedding_dim=embedding_dim,\n",
    "                     mlp_size=mlp_size,\n",
    "                     dropout=dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.msa(x)+x\n",
    "        x=self.mlp(x)+x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting things Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self,\n",
    "                    img_size,\n",
    "                    in_channels,\n",
    "                    patch_size,\n",
    "                    no_of_transformers,\n",
    "                    embedding_dim,\n",
    "                    mlp_size,\n",
    "                    no_heads,\n",
    "                    no_classes,\n",
    "                    dropout=0):\n",
    "        super().__init__()\n",
    "        self.num_patches=(img_size*img_size//patch_size**2)\n",
    "        self.class_ebmedding=nn.Parameter(torch.randn(1,1,embedding_dim),requires_grad=True)\n",
    "        self.positional_embedding=nn.Parameter(torch.randn(1,self.num_patches+1,embedding_dim),requires_grad=True)\n",
    "\n",
    "        self.patch_embedding=PatchEmbedding(input_shape=in_channels,\n",
    "                                            patch_size=patch_size,\n",
    "                                            output_shape=embedding_dim)\n",
    "        \n",
    "        self.transformer=nn.Sequential(*[Transformer(embedding_dim=embedding_dim,\n",
    "                                                    no_heads=no_heads,\n",
    "                                                    mlp_size=mlp_size,\n",
    "                                                    dropout=dropout) for _ in range(no_of_transformers)])\n",
    "        self.classifier=nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            nn.Linear(embedding_dim,no_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        batch_size=x.shape[0]\n",
    "\n",
    "        class_token=self.class_ebmedding.expand(batch_size,-1,-1)\n",
    "        x=self.patch_embedding(x)\n",
    "        x=torch.cat((class_token,x),dim=1)\n",
    "        x +=self.positional_embedding\n",
    "        x=self.transformer(x)\n",
    "        x=self.classifier(x[:,0])\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model=ViT(img_size=SHAPE,\n",
    "            in_channels=COLOR_CHANNEL,\n",
    "            patch_size=PATCH_SIZE,\n",
    "            no_of_transformers=6,\n",
    "            embedding_dim=768,\n",
    "            mlp_size=3072,\n",
    "            no_heads=12,\n",
    "            no_classes=10,\n",
    "            dropout=0.1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================================================================================================\n",
       "Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
       "==========================================================================================================================================================================\n",
       "ViT                                           [1, 3, 224, 224]          [1, 10]                   152,064                   --                        --\n",
       "├─PatchEmbedding: 1-1                         [1, 3, 224, 224]          [1, 196, 768]             --                        --                        --\n",
       "│    └─Conv2d: 2-1                            [1, 3, 224, 224]          [1, 768, 14, 14]          590,592                   [16, 16]                  115,756,032\n",
       "│    └─Flatten: 2-2                           [1, 768, 14, 14]          [1, 768, 196]             --                        --                        --\n",
       "├─Sequential: 1-2                             [1, 197, 768]             [1, 197, 768]             --                        --                        --\n",
       "│    └─Transformer: 2-3                       [1, 197, 768]             [1, 197, 768]             --                        --                        --\n",
       "│    │    └─MSA: 3-1                          [1, 197, 768]             [1, 197, 768]             --                        --                        --\n",
       "│    │    │    └─LayerNorm: 4-1               [1, 197, 768]             [1, 197, 768]             1,536                     --                        1,536\n",
       "│    │    │    └─MultiheadAttention: 4-2      [1, 197, 768]             [1, 197, 768]             2,362,368                 --                        0\n",
       "│    │    └─MLP: 3-2                          [1, 197, 768]             [1, 197, 768]             --                        --                        --\n",
       "│    │    │    └─LayerNorm: 4-3               [1, 197, 768]             [1, 197, 768]             1,536                     --                        1,536\n",
       "│    │    │    └─Sequential: 4-4              [1, 197, 768]             [1, 197, 768]             4,722,432                 --                        4,722,432\n",
       "│    └─Transformer: 2-4                       [1, 197, 768]             [1, 197, 768]             --                        --                        --\n",
       "│    │    └─MSA: 3-3                          [1, 197, 768]             [1, 197, 768]             --                        --                        --\n",
       "│    │    │    └─LayerNorm: 4-5               [1, 197, 768]             [1, 197, 768]             1,536                     --                        1,536\n",
       "│    │    │    └─MultiheadAttention: 4-6      [1, 197, 768]             [1, 197, 768]             2,362,368                 --                        0\n",
       "│    │    └─MLP: 3-4                          [1, 197, 768]             [1, 197, 768]             --                        --                        --\n",
       "│    │    │    └─LayerNorm: 4-7               [1, 197, 768]             [1, 197, 768]             1,536                     --                        1,536\n",
       "│    │    │    └─Sequential: 4-8              [1, 197, 768]             [1, 197, 768]             4,722,432                 --                        4,722,432\n",
       "│    └─Transformer: 2-5                       [1, 197, 768]             [1, 197, 768]             --                        --                        --\n",
       "│    │    └─MSA: 3-5                          [1, 197, 768]             [1, 197, 768]             --                        --                        --\n",
       "│    │    │    └─LayerNorm: 4-9               [1, 197, 768]             [1, 197, 768]             1,536                     --                        1,536\n",
       "│    │    │    └─MultiheadAttention: 4-10     [1, 197, 768]             [1, 197, 768]             2,362,368                 --                        0\n",
       "│    │    └─MLP: 3-6                          [1, 197, 768]             [1, 197, 768]             --                        --                        --\n",
       "│    │    │    └─LayerNorm: 4-11              [1, 197, 768]             [1, 197, 768]             1,536                     --                        1,536\n",
       "│    │    │    └─Sequential: 4-12             [1, 197, 768]             [1, 197, 768]             4,722,432                 --                        4,722,432\n",
       "│    └─Transformer: 2-6                       [1, 197, 768]             [1, 197, 768]             --                        --                        --\n",
       "│    │    └─MSA: 3-7                          [1, 197, 768]             [1, 197, 768]             --                        --                        --\n",
       "│    │    │    └─LayerNorm: 4-13              [1, 197, 768]             [1, 197, 768]             1,536                     --                        1,536\n",
       "│    │    │    └─MultiheadAttention: 4-14     [1, 197, 768]             [1, 197, 768]             2,362,368                 --                        0\n",
       "│    │    └─MLP: 3-8                          [1, 197, 768]             [1, 197, 768]             --                        --                        --\n",
       "│    │    │    └─LayerNorm: 4-15              [1, 197, 768]             [1, 197, 768]             1,536                     --                        1,536\n",
       "│    │    │    └─Sequential: 4-16             [1, 197, 768]             [1, 197, 768]             4,722,432                 --                        4,722,432\n",
       "│    └─Transformer: 2-7                       [1, 197, 768]             [1, 197, 768]             --                        --                        --\n",
       "│    │    └─MSA: 3-9                          [1, 197, 768]             [1, 197, 768]             --                        --                        --\n",
       "│    │    │    └─LayerNorm: 4-17              [1, 197, 768]             [1, 197, 768]             1,536                     --                        1,536\n",
       "│    │    │    └─MultiheadAttention: 4-18     [1, 197, 768]             [1, 197, 768]             2,362,368                 --                        0\n",
       "│    │    └─MLP: 3-10                         [1, 197, 768]             [1, 197, 768]             --                        --                        --\n",
       "│    │    │    └─LayerNorm: 4-19              [1, 197, 768]             [1, 197, 768]             1,536                     --                        1,536\n",
       "│    │    │    └─Sequential: 4-20             [1, 197, 768]             [1, 197, 768]             4,722,432                 --                        4,722,432\n",
       "│    └─Transformer: 2-8                       [1, 197, 768]             [1, 197, 768]             --                        --                        --\n",
       "│    │    └─MSA: 3-11                         [1, 197, 768]             [1, 197, 768]             --                        --                        --\n",
       "│    │    │    └─LayerNorm: 4-21              [1, 197, 768]             [1, 197, 768]             1,536                     --                        1,536\n",
       "│    │    │    └─MultiheadAttention: 4-22     [1, 197, 768]             [1, 197, 768]             2,362,368                 --                        0\n",
       "│    │    └─MLP: 3-12                         [1, 197, 768]             [1, 197, 768]             --                        --                        --\n",
       "│    │    │    └─LayerNorm: 4-23              [1, 197, 768]             [1, 197, 768]             1,536                     --                        1,536\n",
       "│    │    │    └─Sequential: 4-24             [1, 197, 768]             [1, 197, 768]             4,722,432                 --                        4,722,432\n",
       "├─Sequential: 1-3                             [1, 768]                  [1, 10]                   --                        --                        --\n",
       "│    └─LayerNorm: 2-9                         [1, 768]                  [1, 768]                  1,536                     --                        1,536\n",
       "│    └─Linear: 2-10                           [1, 768]                  [1, 10]                   7,690                     --                        7,690\n",
       "==========================================================================================================================================================================\n",
       "Total params: 43,279,114\n",
       "Trainable params: 43,279,114\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 144.12\n",
       "==========================================================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 52.05\n",
       "Params size (MB): 115.81\n",
       "Estimated Total Size (MB): 168.46\n",
       "=========================================================================================================================================================================="
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(1, 3, SHAPE, SHAPE), col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"], depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input=torch.randn(1,3,SHAPE,SHAPE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1228,  0.1758, -1.1480, -0.2348,  0.0632, -0.0963,  0.3380, -0.5666,\n",
      "         -0.3432, -0.1435]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "print(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
